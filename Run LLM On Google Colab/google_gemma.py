# -*- coding: utf-8 -*-
"""google_gemma.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MMZK7ttqt5a5l2kls8F6Gjb_WLDD5a7t
"""

pip install transformers

import os
os.environ["HF_TOKEN"] = ""

from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "google/gemma-3-1b-it"

tokenizer = AutoTokenizer.from_pretrained(model_name)

input_prompt = [
    "The capital of India is"
]

tokenized = tokenizer(input_prompt, return_tensors="pt")

tokenized["input_ids"]

import torch
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    dtype=torch.bfloat16
)

gen_result = model.generate(tokenized["input_ids"], max_new_tokens=25)

gen_result

output = tokenizer.batch_decode(gen_result)
output