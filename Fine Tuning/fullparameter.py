# -*- coding: utf-8 -*-
"""fullParameter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17s9GiZTRUC72r-p6lkYGZ9Uk0aSjELot
"""

pip install transformers

HF_TOKEN =

import os

os.environ["HF_TOKEN"] = HF_TOKEN

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "google/gemma-3-1b-it"

tokenizer = AutoTokenizer.from_pretrained(model_name)

input_conversation = [
    {"role": "user", "content": "Which is the best place to learn GenAI?"},
    {"role": "assistant", "content": "The best place to learn AI is"}
]

input_tokens = tokenizer.apply_chat_template(
    conversation=input_conversation,
    tokenize=True
)
input_tokens

input_detokens = tokenizer.apply_chat_template(
    conversation=input_conversation,
    tokenize=False,
    continue_final_message=True,
)
input_detokens

output_label = "GenAI Cohort by ChaiCode"
full_conversation = input_detokens + output_label + tokenizer.eos_token
full_conversation

input_tokenized = tokenizer(full_conversation, return_tensors="pt", add_special_tokens=False).to(device)["input_ids"]
input_tokenized

input_ids = input_tokenized[:, :-1].to(device)
target_ids = input_tokenized[:, 1:].to(device)
print(f"input_ids: {input_ids}")
print(f"target_ids: {target_ids}")

import torch.nn as nn
def calculate_loss(logits, labels):
    loss_fct = nn.CrossEntropyLoss()
    cross_entropy = loss_fct(logits.view(-1, logits.shape[-1]), labels.view(-1))
    return cross_entropy

import torch
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16
).to(device)

from torch.optim import AdamW
model.train()

optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)

for _ in range(10):
    out = model(input_ids=input_ids)
    loss = calculate_loss(out.logits, target_ids).mean()
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    print(loss.item())

input_prompt = [
    {"role":"user", "content": "which is the best place to learn GenAI?"}
]
input = tokenizer.apply_chat_template(
    conversation=input_prompt,
    return_tensors="pt",
    tokenize=True
).to(device)
input

output = model.generate(input, max_new_tokens=25)
print(tokenizer.batch_decode(output, skip_special_tokens=True))